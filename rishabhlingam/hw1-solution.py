# -*- coding: utf-8 -*-
"""HW1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vlsxUpF90Vy7CGodHKPO4qGRrP9fb1eY
"""

import operator
import numpy as np
from prettytable import PrettyTable

''''''''''''''''''''''''''''''''''''''''
Creation of Grid world (Question-2)

Grid world representation | (s): start | (b): block | (e): end

    0   1   2   3   4   5   6
0   s       b           b
1   b       b
2               b           b
3       b               b
4               b
5           b       b
6    b                       e
'''''''''''''''''''''''''''''''''''''''''

rows, cols = 7, 7
no_states = rows * cols
no_actions = 4
LEFT, UP, RIGHT, DOWN = 0, 1, 2, 3
START = [0]
FINAL = [48]
Blocks = [2, 5, 7, 9, 17, 20, 22, 26, 31, 37, 39, 42]

# Usefull functions
def obtain_row_col_index_0(position, t_rows, t_cols):
    x = position // t_cols
    y = (position % t_cols)
    return x, y

def obtain_row_col_index(position):
    return obtain_row_col_index_0(position, rows, cols)

def obtain_position_0(x, y, t_rows, t_cols):
    x = max(0, min(x, t_rows - 1))
    y = max(0, min(y, t_cols - 1))
    return (x * t_cols) + y

def obtain_position(x, y):
    return obtain_position_0(x, y, rows, cols)

def goto_left(position):
    x, y = obtain_row_col_index(position)
    lef_pos = obtain_position(x, y)
    return (y != 0) and (lef_pos not in Blocks)

def goto_up(position):
    x, y = obtain_row_col_index(position)
    up_pos = obtain_position(x - 1, y)
    return (x != 0) and (up_pos not in Blocks)

def goto_right(position):
    x, y = obtain_row_col_index(position)
    right_pos = obtain_position(x, y + 1)
    return (y != cols - 1) and (right_pos not in Blocks)

def goto_down(position):
    x, y = obtain_row_col_index(position)
    down_pos = obtain_position(x + 1, y)
    return (x != rows - 1) and (down_pos not in Blocks)

def obtain_possible_action_state(position):
    x, y = obtain_row_col_index(position)
    actions, states = [], []

    possible_actions = [(LEFT, (x, y)), (UP, (x - 1, y)), (RIGHT, (x, y + 1)), (DOWN, (x + 1, y))]

    for action, (new_x, new_y) in possible_actions:
        if new_x >= 0 and new_x < rows and new_y >= 0 and new_y < cols and obtain_position(new_x, new_y) not in Blocks:
            actions.append(action)
            states.append(obtain_position(new_x, new_y))

    return actions, states

def obtain_string_policy(policy):
    n = len(policy)
    string_policy = np.chararray(n, itemsize=10, unicode=True)

    # Define a mapping from numeric values to string representations
    action_mapping = {0: '←', 1: '↑', 2: '→', 3: '↓'}

    for s in range(n):
        max_idx = np.argmax(policy[s])
        string_policy[s] = action_mapping[max_idx]

    string_policy[START] = 'S'
    string_policy[FINAL] = 'X'
    string_policy[Blocks] = 'ø'
    return string_policy

def obtain_as_table(policy):
    pTable = PrettyTable(header=False)
    pol_temp = policy.reshape((rows, cols))
    for row in pol_temp:
        pTable.add_row(row, divider=True)
    return pTable

# Transition probability is |S| x |S'| x |A| array
# T[i][j][k]= prob. moving from state i to j when doing action k
# moving out of boundary or to block stays in current state

T = np.zeros((no_states, no_states, no_actions))

for position in range(0, no_states):
        x, y = obtain_row_col_index(position)

        left_pos = obtain_position(x, y)
        up_pos = obtain_position(x - 1, y)
        right_pos = obtain_position(x, y + 1)
        down_pos = obtain_position(x + 1, y)

for position in range(no_states):
    x, y = obtain_row_col_index(position)
    pos_actions, pos_states = obtain_possible_action_state(position)

    for action, state in zip(pos_actions, pos_states):
        T[position, state, action] = 1


# 1-2) probabiistic transition
#  complete T matrix for probabilistic transition

T_prob = np.zeros((no_states, no_states, no_actions))

for position in range(0, no_states):
        x, y = obtain_row_col_index(position)

        left_pos = obtain_position(x, y)
        up_pos = obtain_position(x - 1, y)
        right_pos = obtain_position(x, y + 1)
        down_pos = obtain_position(x + 1, y)

for position in range(no_states):
    x, y = obtain_row_col_index(position)
    pos_actions, pos_states = obtain_possible_action_state(position)

    for action, state in zip(pos_actions, pos_states):
        if action == LEFT:
            T_prob[position, state, action] = np.round(np.random.rand(), 2)
        else:
            T_prob[position, position, action] = 1


# 1-3) Reward function: |S| x |A| array
# R[i][j]= reward from state i and action j
# each move generates -1 reward

R = np.full((no_states, no_actions), -1)
gamma = 0.9

#Policy: |S| x |A| array
#P[i][j]= prob of choosing action j in state i
# 2-1) initialize policy P with uniform policy

P = np.full((no_states, no_actions), 0.25)

# 2-2) implement prediction (policy evaluation)
# compute V values from a policy
# implement prediction(policy evaluation) algorithm in slide page 7.

def policy_eval(policy, max_iter, V=None):
    if V is None:
        V = np.zeros(no_states)

    for _ in range(max_iter):
        V_temp = np.zeros(no_states)

        for s in range(no_states):
            v_s = 0

            for action in range(no_actions):
                q_sa = 0

                for s_prime in range(no_states):
                    q_sa += T[s, s_prime, action] * V[s_prime]

                q_sa *= gamma
                q_sa += R[s, action]
                q_sa *= policy[s, action]
                v_s += q_sa

            V_temp[s] = np.round(v_s, 2)

        V_temp[FINAL] = 0
        V = np.copy(V_temp)

    return V


# 2-3) implement policy improvement with V value using greedy method
# The formula for choosing the best action using V value is given in question.

def extract_policy(V):
    '''
    Procedure to extract a policy from a value function
    pi <-- argmax_a R^a + gamma T^a V

    Inputs:
    V -- Value function: array of |S| entries

    Output:
    policy -- Policy array P
    '''

    # initialize random(uniform) policy
    new_policy = np.zeros((no_states, no_actions))

    for s in range(no_states):
        actions, states = obtain_possible_action_state(s)
        max_idx = np.argmax(V[states])
        max_state = states[max_idx]
        max_act = actions[max_idx]
        new_policy[s, max_act] = 1

    return new_policy


# 2-4) implement policy iteration method
# implement policy iteration in slide page 13

def policy_iter(in_policy, max_iter):

    '''    Policy iteration procedure: alternate between
    1) policy evaluation (solve V^pi = R^pi + gamma T^pi V^pi) and
    2) policy improvement (pi <-- argmax_a R^a + gamma T^a V^pi).

    Inputs:
    in_policy -- Initial policy
    max_iter -- maximum # of iterations: scalar (use a large number)

    Outputs:
    policy -- Policy P
    V -- Value function: array of |S| entries
    no_iter -- the actual # of iterations peformed by policy iteration: scalar
    '''

    # Initialization P and V using np.zeros
    P = in_policy
    V = np.zeros(no_states)
    no_iter = 0

    for i in range(max_iter):
        no_iter += 1
        V = policy_eval(P, 10)
        P_temp = extract_policy(V)

        if np.array_equal(P, P_temp):  # Check for policy convergence
            break

        P = np.copy(P_temp)

    return P, V, no_iter

# 2-5) implement value iteration method
# implement value iteration in Dynamic programming notes page 23

def value_iter(in_V, max_iter):
    '''
    Value iteration procedure
    V <-- max_a R^a + gamma T^a V

    Inputs:
    in_V -- Initial value function: array of |S| entries
    max_iter -- limit on the # of iterations: scalar (use large number)

    Outputs:
    V -- Value function: array of |S| entries
    no_iter -- the actual # of iterations peformed by policy iteration: scalar
    '''

    # Initialization V using np.zeros
    V = in_V
    no_iter = 0

    rand_policy = np.full((no_states, no_actions), 0.25)

    for i in range(max_iter):
        no_iter += 1
        V_temp = policy_eval(rand_policy, 1, V)

        if np.array_equal(V, V_temp):  # Check for value function convergence
            break

        V = np.copy(V_temp)

    return V, no_iter

max_itr = 500

print("VALUE ITERATION")
v_rand = np.round(np.random.rand(no_states), 2)
v_itr, num_iterations = value_iter(v_rand, max_itr)

print("Total Iterations = ", num_iterations)
print("\nState Values:")
state_values_table = obtain_as_table(v_itr)
print(state_values_table)

print("\nOptimal Policy:")
optimal_policy = extract_policy(v_itr)
policy_table = obtain_as_table(obtain_string_policy(optimal_policy))
print(policy_table)
print('\n')

max_itr = 500

print("POLICY ITERATION")
initial_policy = np.full((no_states, no_actions), 0.25)
optimal_policy, state_values, num_iterations = policy_iter(initial_policy, max_itr)

print("Total Iterations = ", num_iterations)
print("\nState Values:")
state_values_table = obtain_as_table(state_values)
print(state_values_table)

print("\nOptimal Policy:")
policy_table = obtain_as_table(obtain_string_policy(optimal_policy))
print(policy_table)
print('\n')